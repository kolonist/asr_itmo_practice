{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0eabe",
   "metadata": {},
   "source": [
    "# Языковые модели\n",
    "\n",
    "Языковые модели играют важную роль в системах распознавания речи, помогая создавать более грамотные и лексически корректные тексты. В данной работе мы будем изучать нграмные языковые модели, которые позволяют довольно легко оценить вероятность и правдоподобность текста.\n",
    "\n",
    "В нграмной языковой модели, нграм - это последовательность из n слов в тексте. Например, в предложении \"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\", биграмами будут \"по-моему мы\", \"мы сэкономим\", \"сэкономим уйму\" итд. Языковые модели оценивают вероятность появления последовательности слов, исходя из статистики появления каждого из нграм в обучающей выборке.\n",
    "\n",
    "Порядком (order) нграм языковой модели называют максимальную длину нграм, которую учитывает модель. \n",
    "\n",
    "Практическая работа разделена на 2 части: \n",
    "1. Построение нграмой языковой модели - основная часть, 12 баллов\n",
    "1. Предсказание с помощью языковой модели - дополнительная часть, 4 балла\n",
    "\n",
    "\n",
    "\n",
    "Полезные сслыки:\n",
    "* arpa формат - https://cmusphinx.github.io/wiki/arpaformat/\n",
    "* обучающие материалы - https://pages.ucsd.edu/~rlevy/teaching/2015winter/lign165/lectures/lecture13/lecture13_ngrams_with_SRILM.pdf\n",
    "* обучающие материалы.2 - https://cjlise.github.io/machine-learning/N-Gram-Language-Model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd5c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21c8e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1c1d7",
   "metadata": {},
   "source": [
    "# 1. Построение нграмной языковой модели. (12 баллов)\n",
    "\n",
    "\n",
    "Вероятность текста с помощью нграмной языковой модели можно вычислить по формуле: \n",
    "$$ P(w_1, w_2, .., w_n) = {\\prod{{P_{i=0}^{n}(w_i| w_{i-order}, .., w_{i-1})}}} $$\n",
    "\n",
    "В простом виде, при обучении нграмной языковой модели, чтобы рассчитать условную вероятность каждой нграмы, используется формула, основанная на количестве появлений нграмы в обучающей выборке. Формула выглядит следующим образом:\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i})} \\over {count(w_{i-order},..., w_{i-1})}} $$\n",
    "\n",
    "Поскольку униграмы не содержат в себе какого-дибо контекста, вероятность униграмы можно посчитать поделив кол-во этой слова на общее количество слов в обучающей выборке. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5837fe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('привет',): 6, ('привет', 'привет'): 5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# в первую очередь нам понадобится подсчитать статистику по обучающей выборке \n",
    "def count_ngrams(train_text: List[str], order=3, bos=True, eos=True) -> Dict[Tuple[str], int]:\n",
    "    ngrams = defaultdict(int)\n",
    "\n",
    "    # TODO реализуйте функцию, которая подсчитывает все 1-gram 2-gram ... order-gram ngram'ы в тексте\n",
    "\n",
    "    for text in train_text:\n",
    "        tokens = re.split(r\"\\s+\", text.strip())\n",
    "\n",
    "        if bos:\n",
    "            tokens = [\"<s>\"] + tokens\n",
    "\n",
    "        if eos:\n",
    "            tokens = tokens + [\"</s>\"]\n",
    "\n",
    "        for n in range(0, order):\n",
    "            for pos in range(0, len(tokens)):\n",
    "                ngram = tuple(tokens[pos:pos+n+1])\n",
    "\n",
    "                if len(ngram) < n + 1:\n",
    "                    continue\n",
    "\n",
    "                if ngram not in ngrams:\n",
    "                    ngrams[ngram] = 0\n",
    "\n",
    "                ngrams[ngram] += 1\n",
    "\n",
    "    return dict(ngrams)\n",
    "\n",
    "# debug run\n",
    "#count_ngrams(['привет привет как дела'], order=2, bos=True, eos=True)\n",
    "count_ngrams(\n",
    "    [\n",
    "        'практическое сентября',\n",
    "        'второе практическое занятие пройдет в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "        'в офлайне в 32 12'\n",
    "    ], \n",
    "    order=5\n",
    ")\n",
    "count_ngrams(['привет ' * 6], order=2, bos=False, eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd69d44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1a passed\n"
     ]
    }
   ],
   "source": [
    "def test_count_ngrams():\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=True, eos=True) == {\n",
    "        ('<s>',): 1, \n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1, \n",
    "        ('</s>',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=True) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1, \n",
    "        ('</s>',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=False) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=2, bos=False, eos=False) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1,\n",
    "        ('привет', 'привет'): 1,\n",
    "        ('привет', 'как'): 1,\n",
    "        ('как', 'дела'): 1\n",
    "    }    \n",
    "    assert count_ngrams(['привет ' * 6], order=2, bos=False, eos=False) == {\n",
    "        ('привет',): 6, \n",
    "        ('привет', 'привет'): 5\n",
    "    }\n",
    "    result = count_ngrams(['практическое сентября',\n",
    "                           'второе практическое занятие пройдет в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "                           'в офлайне в 32 12'], order=5)\n",
    "    assert result[('<s>',)] == 3\n",
    "    assert result[('32',)] == 3\n",
    "    assert result[('<s>', 'в', 'офлайне', 'в', '32')] == 1\n",
    "    assert result[('офлайне', 'в', '32', '12', '</s>')] == 1\n",
    "    print('Test 1a passed')\n",
    "    \n",
    "    \n",
    "test_count_ngrams()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e1865",
   "metadata": {},
   "source": [
    "\n",
    "Простой подход к вычислению вероятностей через количество нграм имеет существенный недостаток. Если в тексте встретится нграмма, которой не было в обучающей выборке, то вероятность всего текста будет равна нулю. \n",
    "\n",
    "Чтобы избежать данного недостатка, вводится специальное сглаживание - [сглаживание Лапласа](https://en.wikipedia.org/wiki/Additive_smoothing). Данная техника позволяет учитывать нграмы, не встретившиеся в обучающей выборке, и при этом не делает вероятность текста равной нулю.\n",
    "\n",
    "Формула сглаживания Лапласа выглядит следующим образом:\n",
    "\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i}) + k} \\over {count(w_{i-order},..., w_{i-1}) + k*V}} $$\n",
    "\n",
    "Здесь V - количество слов в словаре, а k - гиперпараметр, который контролирует меру сглаживания. Как правило, значение k выбирается экспериментально, чтобы найти оптимальный баланс между учетом редких нграм и сохранением вероятности для часто встречающихся нграм.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cafb4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n",
      "0.16 0.3333333333333333\n",
      "0.14516129032258066 0.25\n",
      "1.0 1.0\n",
      "0.1875 0.1875\n",
      "0.25 0.25\n",
      "0.0 0.0\n",
      "0.0625 0.0625\n",
      "0.25 0.25\n"
     ]
    }
   ],
   "source": [
    "# функция подсчета вероятности через количество со сглаживанием Лапласа\n",
    "def calculate_ngram_prob(ngram: Tuple[str], counts: Dict[Tuple[str], int], V=None, k=0) -> float:\n",
    "    # подсчитывет ngram со сглаживанием Лапласа\n",
    "    # TODO\n",
    "\n",
    "    ngram_count = counts[ngram] if ngram in counts else 0\n",
    "\n",
    "    n = len(ngram)\n",
    "\n",
    "    rest_count = 0\n",
    "    voc_size = 0\n",
    "    for ngram_text, count in counts.items():\n",
    "        if V is None and len(ngram_text) == 1:\n",
    "            voc_size += 1\n",
    "\n",
    "        if len(ngram_text) == n and ngram_text[:-1] == ngram[:-1]:\n",
    "            rest_count += count\n",
    "\n",
    "    if V is None:\n",
    "        V = voc_size\n",
    "\n",
    "    prob = (ngram_count + k) / (rest_count + (k * V))\n",
    "\n",
    "    return prob\n",
    "\n",
    "# debug run\n",
    "counts = count_ngrams(\n",
    "    [\n",
    "        'практическое сентября',\n",
    "        'второе практическое занятие в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "        'в офлайне в 32 12'\n",
    "    ],\n",
    "    order=4\n",
    ")\n",
    "#pprint(counts)\n",
    "\n",
    "print(calculate_ngram_prob(('в', 'офлайне'), counts), 0.5)\n",
    "print(calculate_ngram_prob(('в', ), counts), 4/12)\n",
    "print(calculate_ngram_prob(('в', ), counts, k=0.5), 0.25)\n",
    "print(calculate_ngram_prob(('в', 'офлайне', 'в', '32'), counts), 1.0)\n",
    "print(calculate_ngram_prob(('в', 'офлайне'), counts, k=1), 0.1875)\n",
    "print(calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5), 0.25)\n",
    "print(calculate_ngram_prob(('в', 'онлайне'), counts, k=0), 0.0)\n",
    "print(calculate_ngram_prob(('в', 'онлайне'), counts, k=1), 0.0625)\n",
    "print(calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5), 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b25d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.b passed\n"
     ]
    }
   ],
   "source": [
    "def test_calculate_ngram_prob():\n",
    "    counts = count_ngrams(['практическое сентября',\n",
    "                           'второе практическое занятие в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "                           'в офлайне в 32 12'], order=4)\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts) == 0.5\n",
    "    assert calculate_ngram_prob(('в', ), counts) == 4/25\n",
    "    assert calculate_ngram_prob(('в', ), counts, k=0.5) == (4+0.5)/(25+0.5*12)\n",
    "    assert calculate_ngram_prob(('в', 'офлайне', 'в', '32'), counts) == 1.0\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=1) == 0.1875\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
    "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=0) == 0.0\n",
    "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=1) == 0.0625\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
    "\n",
    "    print(\"Test 1.b passed\")\n",
    "    \n",
    "\n",
    "test_calculate_ngram_prob()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da494bf0",
   "metadata": {},
   "source": [
    "Основной метрикой язковых моделей является перплексия. \n",
    "\n",
    "Перплексия  — безразмерная величина, мера того, насколько хорошо распределение вероятностей предсказывает выборку. Низкий показатель перплексии указывает на то, что распределение вероятности хорошо предсказывает выборку.\n",
    "\n",
    "$$ ppl = {P(w_1, w_2 ,..., w_N)^{- {1} \\over {N}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd1f2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lm.V = 14\n",
      "\n",
      "lm.predict_log_proba(['мы']) = -7.594318331665067\n",
      "lm.predict_log_proba([\"если\"]) = -7.594318331665067\n",
      "lm.predict_log_proba([\"по-моему\"]) = -6.901171151105122\n",
      "\n",
      "lm.ppl(['']) = 15.288884851420656 / 10.908712114635714\n",
      "lm.ppl(['ЧТО']) = 14.846584407951667 / 11.854729962672497\n",
      "lm.ppl(test_data) = 7.334561964590594 / 6.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Языковая модель \n",
    "class NgramLM:\n",
    "    def __init__(self, order=3, bos=True, eos=True, k=1, predefined_vocab=None):\n",
    "        self.order = order\n",
    "        self.eos = eos\n",
    "        self.bos = bos\n",
    "        self.k = k\n",
    "        self.vocab = predefined_vocab\n",
    "        self.ngrams_count = None\n",
    "        \n",
    "    @property\n",
    "    def V(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def _tokenize(self, text:str) -> list[str]:\n",
    "        return re.split(r\"\\s+\", text.strip())\n",
    "\n",
    "    def _count_ngrams(self, train_text: List[str]) -> Dict[Tuple[str], int]:\n",
    "        ngrams = defaultdict(int)\n",
    "\n",
    "        for text in train_text:\n",
    "            tokens = self._tokenize(text)\n",
    "\n",
    "            if self.bos:\n",
    "                tokens = [\"<s>\"] + tokens\n",
    "\n",
    "            if self.eos:\n",
    "                tokens = tokens + [\"</s>\"]\n",
    "\n",
    "            for n in range(0, self.order):\n",
    "                for pos in range(0, len(tokens)):\n",
    "                    ngram = tuple(tokens[pos:pos+n+1])\n",
    "\n",
    "                    if len(ngram) < n + 1:\n",
    "                        continue\n",
    "\n",
    "                    if ngram not in ngrams:\n",
    "                        ngrams[ngram] = 0\n",
    "\n",
    "                    ngrams[ngram] += 1\n",
    "\n",
    "        return dict(ngrams)\n",
    "\n",
    "    def _calculate_ngram_proba(self, ngram: Tuple[str]) -> float:\n",
    "\n",
    "        ngram_count = self.ngrams_count[ngram] if ngram in self.ngrams_count else 0\n",
    "\n",
    "        n = len(ngram)\n",
    "\n",
    "        rest_count = 0\n",
    "        for ngram_text, count in self.ngrams_count.items():\n",
    "            if len(ngram_text) == n and ngram_text[:-1] == ngram[:-1]:\n",
    "                rest_count += count\n",
    "\n",
    "        prob = (ngram_count + self.k) / (rest_count + (self.k * self.V))\n",
    "\n",
    "        return prob\n",
    "\n",
    "    def fit(self, train_text: List[str]) -> None:\n",
    "        # TODO\n",
    "        # Подсчет vocab и ngrams_count по обучающей выборке\n",
    "        self.ngrams_count = self._count_ngrams(train_text)\n",
    "        self.vocab = [ ngram for ngram in self.ngrams_count.keys() if len(ngram) == 1 ]\n",
    "\n",
    "    def predict_ngram_log_proba(self, ngram: Tuple[str]) -> float:\n",
    "        # TODO \n",
    "        # считаем логарифм вероятности конкретной нграмы\n",
    "        proba = self._calculate_ngram_proba(ngram)\n",
    "        return np.log(proba)\n",
    "\n",
    "    def predict_log_proba(self, words: List[str], return_ngrams_count:bool=False) -> float:\n",
    "        if self.bos:\n",
    "            words = ['<s>'] + words\n",
    "        if self.eos:\n",
    "            words = words + ['</s>']\n",
    "        logprob = 0\n",
    "        # TODO \n",
    "        # применяем chain rule, чтобы посчитать логарифм вероятности всей строки\n",
    "\n",
    "        words = [ word for word in words if word.strip() != \"\" ]\n",
    "\n",
    "        start = 1 - self.order\n",
    "        end = 0\n",
    "        ngrams_count = 0\n",
    "        while end < len(words):\n",
    "            actual_start = start if start > 0 else 0\n",
    "            actual_end = end + 1\n",
    "\n",
    "            ngram = tuple(words[actual_start:actual_end])\n",
    "\n",
    "            logprob += self.predict_ngram_log_proba(ngram)\n",
    "\n",
    "            start += 1\n",
    "            end += 1\n",
    "\n",
    "            ngrams_count += 1\n",
    "\n",
    "        return logprob if not return_ngrams_count else (logprob, ngrams_count)\n",
    "\n",
    "    def ppl(self, text: List[str]) -> float:\n",
    "        #TODO \n",
    "        # подсчет перплексии\n",
    "        # Для того, чтобы ваш код был численно стабильным, \n",
    "        #    не считайте формулу напрямую, а воспользуйтесь переходом к логарифмам вероятностей\n",
    "        # \n",
    "        perplexity = 0\n",
    "        n = 0\n",
    "\n",
    "        for line in text:\n",
    "            tokens = self._tokenize(line)\n",
    "            log_proba, ngrams_count = self.predict_log_proba(tokens, return_ngrams_count=True)\n",
    "            perplexity += log_proba\n",
    "            n += ngrams_count\n",
    "\n",
    "        perplexity = np.exp(-perplexity / n)\n",
    "\n",
    "        return perplexity\n",
    "\n",
    "# debur run\n",
    "train_data = [\n",
    "    \"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\",\n",
    "    \"если я сойду с ума прямо сейчас по-моему мы сэкономим уйму времени\",\n",
    "    \"мы сэкономим уйму времени если я сейчас сойду с ума по-моему\"\n",
    "]\n",
    "\n",
    "lm = NgramLM(order=2)\n",
    "lm.fit(train_data)\n",
    "\n",
    "test_data = [\"по-моему если я прямо сейчас сойду с ума мы сэкономим уйму времени\"]\n",
    "\n",
    "print(f\"\"\"\n",
    "lm.V = {lm.V}\n",
    "\n",
    "lm.predict_log_proba(['мы']) = {lm.predict_log_proba(['мы'])}\n",
    "lm.predict_log_proba([\"если\"]) = {lm.predict_log_proba([\"если\"])}\n",
    "lm.predict_log_proba([\"по-моему\"]) = {lm.predict_log_proba([\"по-моему\"])}\n",
    "\n",
    "lm.ppl(['']) = {lm.ppl([''])} / {((3+1)/28 * 1/(3+14))**(-1/2)}\n",
    "lm.ppl(['ЧТО']) = {lm.ppl(['ЧТО'])} / {((3+1)/28 * 1/(3+14) * 1/(14)) ** (-1/3)}\n",
    "lm.ppl(test_data) = {lm.ppl(test_data)} / 6.99\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0bfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lm():\n",
    "    train_data = [\"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\",\n",
    "                  \"если я сойду с ума прямо сейчас по-моему мы сэкономим уйму времени\",\n",
    "                  \"мы сэкономим уйму времени если я сейчас сойду с ума по-моему\"]\n",
    "    global lm\n",
    "    lm = NgramLM(order=2)\n",
    "    lm.fit(train_data)\n",
    "    assert lm.V == 14\n",
    "    assert np.isclose(lm.predict_log_proba(['мы']), lm.predict_log_proba([\"если\"]))\n",
    "    assert lm.predict_log_proba([\"по-моему\"]) > lm.predict_log_proba([\"если\"]) \n",
    "    \n",
    "    gt = ((3+1)/(41 + 14) * 1/(3+14))**(-1/2)\n",
    "    ppl = lm.ppl([''])\n",
    "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
    "    \n",
    "    gt = ((3+1)/(41 + 14) * 1/(3+14) * 1/(14)) ** (-1/3)\n",
    "    ppl = lm.ppl(['ЧТО'])\n",
    "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
    "    \n",
    "    test_data = [\"по-моему если я прямо сейчас сойду с ума мы сэкономим уйму времени\"]\n",
    "    ppl = lm.ppl(test_data)\n",
    "    assert round(ppl, 2) == 7.33, f\"{ppl}\"\n",
    "test_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafa0a2",
   "metadata": {},
   "source": [
    "# 2. Предсказания с помощью языковой модели (4 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d2eb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('мы', -1.7346010553881064),\n",
       " ('</s>', -2.1400661634962708),\n",
       " ('<s>', -2.833213344056216),\n",
       " ('по-моему', -2.833213344056216)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_next_word(lm: NgramLM, prefix: List[str], topk=4):\n",
    "    # TODO реализуйте функцию, которая предсказывает продолжение фразы. \n",
    "    # верните topk наиболее вероятных продолжений фразы prefix \n",
    "\n",
    "    probabilities = []\n",
    "\n",
    "    for word in lm.vocab:\n",
    "        ngram = tuple(prefix + list(word))\n",
    "        proba = lm.predict_ngram_log_proba(ngram)\n",
    "\n",
    "        probabilities.append({\n",
    "            \"ngram\": ngram,\n",
    "            \"proba\": proba,\n",
    "        })\n",
    "\n",
    "    probabilities.sort(key=lambda row: row[\"proba\"], reverse=True)\n",
    "\n",
    "    next_words = [ (row[\"ngram\"][-1], row[\"proba\"]) for row in probabilities[:topk] ]\n",
    "\n",
    "    return next_words\n",
    "\n",
    "\n",
    "predict_next_word(lm, [\"по-моему\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4846b",
   "metadata": {},
   "source": [
    "Обучите языковую модель на всем тексте из этой лабораторной работы (не забудьте заранее нормализовать текст).\n",
    "\n",
    "Что предскажет модель по префиксам `по-моему`, `сэкономим`, `нграм` и `вероятности`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a7daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = \"\"\"\n",
    "# Языковые модели\n",
    "\n",
    "Языковые модели играют важную роль в системах распознавания речи, помогая создавать более грамотные и лексически корректные тексты. В данной работе мы будем изучать нграмные языковые модели, которые позволяют довольно легко оценить вероятность и правдоподобность текста.\n",
    "\n",
    "В нграмной языковой модели, нграм - это последовательность из n слов в тексте. Например, в предложении \"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\", биграмами будут \"по-моему мы\", \"мы сэкономим\", \"сэкономим уйму\" итд. Языковые модели оценивают вероятность появления последовательности слов, исходя из статистики появления каждого из нграм в обучающей выборке.\n",
    "\n",
    "Порядком (order) нграм языковой модели называют максимальную длину нграм, которую учитывает модель. \n",
    "\n",
    "Практическая работа разделена на 2 части: \n",
    "1. Построение нграмой языковой модели - основная часть, 12 баллов\n",
    "1. Предсказание с помощью языковой модели - дополнительная часть, 4 балла\n",
    "\n",
    "\n",
    "\n",
    "Полезные сслыки:\n",
    "* arpa формат - https://cmusphinx.github.io/wiki/arpaformat/\n",
    "* обучающие материалы - https://pages.ucsd.edu/~rlevy/teaching/2015winter/lign165/lectures/lecture13/lecture13_ngrams_with_SRILM.pdf\n",
    "* обучающие материалы.2 - https://cjlise.github.io/machine-learning/N-Gram-Language-Model/\n",
    "\n",
    "# 1. Построение нграмной языковой модели. (12 баллов)\n",
    "\n",
    "\n",
    "Вероятность текста с помощью нграмной языковой модели можно вычислить по формуле: \n",
    "$$ P(w_1, w_2, .., w_n) = {\\prod{{P_{i=0}^{n}(w_i| w_{i-order}, .., w_{i-1})}}} $$\n",
    "\n",
    "В простом виде, при обучении нграмной языковой модели, чтобы рассчитать условную вероятность каждой нграмы, используется формула, основанная на количестве появлений нграмы в обучающей выборке. Формула выглядит следующим образом:\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i})} \\over {count(w_{i-order},..., w_{i-1})}} $$\n",
    "\n",
    "\n",
    "Простой подход к вычислению вероятностей через количество нграм имеет существенный недостаток. Если в тексте встретится нграмма, которой не было в обучающей выборке, то вероятность всего текста будет равна нулю. \n",
    "\n",
    "Чтобы избежать данного недостатка, вводится специальное сглаживание - [сглаживание Лапласа](https://en.wikipedia.org/wiki/Additive_smoothing). Данная техника позволяет учитывать нграмы, не встретившиеся в обучающей выборке, и при этом не делает вероятность текста равной нулю.\n",
    "\n",
    "Формула сглаживания Лапласа выглядит следующим образом:\n",
    "\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i}) + k} \\over {count(w_{i-order},..., w_{i-1}) + k*V}} $$\n",
    "\n",
    "Здесь V - количество слов в словаре, а k - гиперпараметр, который контролирует меру сглаживания. Как правило, значение k выбирается экспериментально, чтобы найти оптимальный баланс между учетом редких нграм и сохранением вероятности для часто встречающихся нграм.\n",
    "\n",
    "Основной метрикой язковых моделей является перплексия. \n",
    "\n",
    "Перплексия  — безразмерная величина, мера того, насколько хорошо распределение вероятностей предсказывает выборку. Низкий показатель перплексии указывает на то, что распределение вероятности хорошо предсказывает выборку.\n",
    "\n",
    "$$ ppl = {P(w_1, w_2 ,..., w_N)^{- {1} \\over {N}}} $$\n",
    "\n",
    "# 2. Предсказания с помощью языковой модели (4 балла)\n",
    "\n",
    "Обучите языковую модель на всем тексте из этой лабораторной работы (не забудьте заранее нормализовать текст).\n",
    "\n",
    "Что предскажет модель по префиксам `по-моему`, `сэкономим`, `нграм` и `вероятности`? \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d215c",
   "metadata": {},
   "source": [
    "Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbe52ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# приводим к нижнему регистру\n",
    "normalized = src_text.strip().lower()\n",
    "\n",
    "# удаляем строки с формулами\n",
    "normalized = re.sub(r\"^\\s*\\$\\$.+\\$\\$\\s*$\", \"\", normalized, flags=re.MULTILINE)\n",
    "\n",
    "# удаляем всё, кроме разделителей предложений, цифр и букв\n",
    "normalized = re.sub(r\"[^\\w\\d\\.\\?!\\-\\s]\", \" \", normalized)\n",
    "\n",
    "# удаляем тире и лишние пробелы\n",
    "normalized = re.sub(r\"\\s+-\\s+\", \" \", normalized)\n",
    "normalized = re.sub(r\"\\s+\", \" \", normalized)\n",
    "\n",
    "# делим на предложения\n",
    "sentences = re.split(r\"[\\.!?]+|\\n+\", normalized)\n",
    "\n",
    "# удаляем лишние пробелы\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip() != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "107862fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "по-моему\n",
      "[('мы', -4.382026634673881),\n",
      " ('сэкономим', -4.787491742782046),\n",
      " ('<s>', -5.480638923341991),\n",
      " ('языковые', -5.480638923341991)]\n",
      "\n",
      "сэкономим\n",
      "[('уйму', -4.386184644822546),\n",
      " ('нграм', -4.791649752930709),\n",
      " ('сэкономим', -4.791649752930709),\n",
      " ('<s>', -5.484796933490655)]\n",
      "\n",
      "нграм\n",
      "[('и', -4.402645921876617),\n",
      " ('в', -4.808111029984782),\n",
      " ('</s>', -4.808111029984782),\n",
      " ('языковой', -4.808111029984782)]\n",
      "\n",
      "вероятности\n",
      "[('</s>', -4.787491742782046),\n",
      " ('для', -4.787491742782046),\n",
      " ('хорошо', -4.787491742782046),\n",
      " ('<s>', -5.480638923341991)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = sentences\n",
    "\n",
    "lm = NgramLM(order=2)\n",
    "lm.fit(train_data)\n",
    "\n",
    "\n",
    "print(\"по-моему\")\n",
    "pprint(predict_next_word(lm, [\"по-моему\"], topk=4))\n",
    "print()\n",
    "\n",
    "print(\"сэкономим\")\n",
    "pprint(predict_next_word(lm, [\"сэкономим\"], topk=4))\n",
    "print()\n",
    "\n",
    "print(\"нграм\")\n",
    "pprint(predict_next_word(lm, [\"нграм\"], topk=4))\n",
    "print()\n",
    "\n",
    "print(\"вероятности\")\n",
    "pprint(predict_next_word(lm, [\"вероятности\"], topk=4))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13e198",
   "metadata": {},
   "source": [
    "Попробуем сгенерировать короткий текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d861540d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'для часто встречающихся нграм и лексически корректные тексты </s> <s> языковые'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word = \"для\"\n",
    "length = 10\n",
    "\n",
    "text = first_word\n",
    "next_word = first_word\n",
    "\n",
    "for _ in range(0, length):\n",
    "    next_word = predict_next_word(lm, [next_word], topk=1)[0][0]\n",
    "    text += \" \" + next_word\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adb95dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'что распределение вероятностей через количество нграм и лексически корректные тексты </s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word = \"что\"\n",
    "length = 10\n",
    "\n",
    "text = first_word\n",
    "next_word = first_word\n",
    "\n",
    "for _ in range(0, length):\n",
    "    next_word = predict_next_word(lm, [next_word], topk=1)[0][0]\n",
    "    text += \" \" + next_word\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6699c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'по-моему мы сэкономим уйму времени если в обучающей выборке </s> <s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word = \"по-моему\"\n",
    "length = 10\n",
    "\n",
    "text = first_word\n",
    "next_word = first_word\n",
    "\n",
    "for _ in range(0, length):\n",
    "    next_word = predict_next_word(lm, [next_word], topk=1)[0][0]\n",
    "    text += \" \" + next_word\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246c197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
